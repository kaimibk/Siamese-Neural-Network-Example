{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "378497b1-2fd6-43a0-bc49-61c5a0d20718",
   "metadata": {},
   "source": [
    "# Using Siamese Networks for \"Classification\"\n",
    "\n",
    "Here we adopt a Siamese Neural Network (SNNs) algorithm to perform \"classification.\" SNNs are a powerful tool for performing similarity analysis, and are very powerful when you have severely unbalanced data. With this technique, a single training example is not a single record, instead you feed in a pair-wise input of two different records, and ask how similar they are. In this notebook, we will take all records and generate every combination of these records (`X_train`) and whether or not they belong to the same class (`y_train`) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a09f3a5-cab7-4a30-a189-13cdcd72f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 23:32:12.241435: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-17 23:32:12.241486: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-17 23:32:12.241508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-17 23:32:12.247600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c1aba-5da6-4dc5-b354-65106da425c2",
   "metadata": {},
   "source": [
    "## Load Your Data\n",
    "\n",
    "Replace the following with your actual data. In this example, we are looking at a binary classification problem, however we could (AND SHOULD) instead change this a multi-classification problem across all traits - moreon this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4699ac-76c6-47cc-93e1-1929b14c6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Don't load all of your data! You will run out of memory!\n",
    "features = np.array([\n",
    "    [0,0,1,1,1,0,0,0,1,1,1,0,None,1.0,1,0,0,1.0,1,0,0,3.0,2.0,1.0,0.0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,2,0,2,0,2,2,2,0,1,0.0,1.0,1,0,0,1.0,0,1,0,1.0,2.0,1.0,0.0,1,0,0,0,0,0,0,0],\n",
    "    [0,2,0,1,1,2,0,1,2,1,0,1,0.0,1.0,1,0,0,0.0,1,0,0,0.0,1.0,0.0,1.0,1,0,0,0,0,0,0,0],\n",
    "    [0,0,0,1,1,2,0,2,0,1,0,1,3.0,1.0,0,1,0,1.0,1,0,0,3.0,2.0,0.0,1.0,0,1,0,0,0,0,0,0],\n",
    "    [0,0,0,0,2,1,0,2,2,1,0,1,1.0,1.0,1,0,0,1.0,1,0,0,3.0,2.0,1.0,0.0,1,0,0,1,0,0,0,0],\n",
    "    [0,1,1,1,1,0,0,2,1,2,0,1,4.0,0.0,1,0,0,None,1,0,0,None,None,None,None,0,1,0,1,0,0,0,0],\n",
    "    [1,0,0,0,1,2,1,1,1,0,0,1,1.0,0.0,1,0,0,1.0,1,0,0,0.0,2.0,0.0,0.0,1,0,0,0,0,0,0,0],\n",
    "    [0,2,0,0,2,1,0,2,1,0,1,0,None,0.0,1,0,0,0.0,0,1,0,0.0,2.0,1.0,0.0,1,0,0,0,0,0,0,0],\n",
    "    [1,0,1,1,1,1,0,1,1,0,1,0,None,0.0,1,0,0,None,0,1,0,None,None,None,None,1,0,0,0,1,0,0,0],\n",
    "    [0,1,1,0,2,1,2,1,0,1,0,1,2.0,1.0,1,0,0,None,0,1,0,None,None,None,None,1,0,0,0,0,0,0,0]\n",
    "])\n",
    "\n",
    "# HACK: For now, map `None` to -1. However, some thought may be required on appropriate representation.\n",
    "# As an example, instead of representing answers to an optional question as YES/NO/BLANK (1/0/None) you can instead adopt (1,-1,0)\n",
    "features[features==None] = -1\n",
    "\n",
    "labels = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4a14e-a41e-4b0f-9dda-cf8ab8d42eb2",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Next step is to construct the actual training data, by generating **all combinations** of our samples (`pair_data`) and if they belong to the same class (`pair_labels`).\n",
    "\n",
    "You can see how this easily extends beyond binary classification/similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112a6e8a-a44e-408b-a6a9-036eaf717077",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_data = []\n",
    "pair_labels = []\n",
    "\n",
    "# Generate pairs\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        pair_data.append([features[i], features[j]])\n",
    "        pair_labels.append(labels[i] == labels[j])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "pair_data = np.array(pair_data)\n",
    "pair_data = pair_data.astype(int)\n",
    "pair_labels = np.array(pair_labels, dtype=int)\n",
    "\n",
    "# Split your dataset into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(pair_data, pair_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14e099-31a3-401f-9b87-67187f0a8805",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture\n",
    "\n",
    "Here is an initial Siamese Neural Network with a very simple architecture. As with any other neural network project, you should spend considerable time tuning the hyperparameters and infrastructure.\n",
    "\n",
    "For example, you borrow lessons learend from architectures likes ResNet or ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c875815a-f08c-461b-8c03-b038f7341e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 23:32:14.595741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:14.599699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:14.599733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:14.601533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:14.601569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:14.601589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:15.270270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:15.270316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:15.270325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-17 23:32:15.270353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-17 23:32:15.270370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8887 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train[0][0].shape\n",
    "\n",
    "# Define Siamese network architecture\n",
    "def create_siamese_network():\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(input_layer)  # CHANGE ME\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)  # CHANGE ME\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)  # CHANGE ME\n",
    "    output_layer = tf.keras.layers.Dense(128)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Create two input layers for pairs of data\n",
    "input_left = tf.keras.layers.Input(shape=input_shape)\n",
    "input_right = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "# Build individual Siamese networks\n",
    "siamese_network = create_siamese_network()\n",
    "embedding_left = siamese_network(input_left)\n",
    "embedding_right = siamese_network(input_right)\n",
    "\n",
    "# Define the contrastive loss function\n",
    "# NOTE: there are other loss functions you can use\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Cast y_true to float32\n",
    "    margin = 1.0\n",
    "    return tf.reduce_mean(y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0)))\n",
    "\n",
    "# Build the Siamese model\n",
    "model = tf.keras.models.Model(inputs=[input_left, input_right], outputs=[embedding_left, embedding_right])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=contrastive_loss)  # CHANGE ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cff04-0e5b-4cf0-a34f-0167c63c1c9b",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Crude model training. Again, you should tune hyperparamters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221a8754-1d27-444f-8cee-d5428a34bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 23:32:18.605475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbb88268210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-17 23:32:18.605514: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-10-17 23:32:18.611281: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-17 23:32:18.748009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2023-10-17 23:32:18.831441: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 4s 24ms/step - loss: 1.1798 - model_loss: 0.5846 - model_1_loss: 0.5952\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 1.0274 - model_loss: 0.5050 - model_1_loss: 0.5225\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8944 - model_loss: 0.4350 - model_1_loss: 0.4593\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.7755 - model_loss: 0.3714 - model_1_loss: 0.4041\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6675 - model_loss: 0.3148 - model_1_loss: 0.3527\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.5935 - model_loss: 0.2768 - model_1_loss: 0.3167\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5934 - model_loss: 0.2778 - model_1_loss: 0.3156\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.6044 - model_loss: 0.2828 - model_1_loss: 0.3216\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5833 - model_loss: 0.2700 - model_1_loss: 0.3132\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5330 - model_loss: 0.2422 - model_1_loss: 0.2908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbc3ecedb50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Siamese network\n",
    "batch_size = 32  # CHANGE ME\n",
    "num_epochs = 10  # CHANGE ME\n",
    "\n",
    "model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de30fda-8afd-427e-93dc-7724c9f94264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step - loss: 0.5983 - model_loss: 0.2803 - model_1_loss: 0.3180\n",
      "Test loss: [0.5983409285545349, 0.28031328320503235, 0.31802764534950256]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73a9c3-3ffd-4c07-b9d2-932ede109444",
   "metadata": {},
   "source": [
    "## Inference Example\n",
    "\n",
    "Here we grab a random record from our data to serve as a theoretical `new_data_sample`. Similarly, we grab 2 random case records to serve as `cases_reference_samples`.\n",
    "\n",
    "We then compare the `new_data_sample` to each of the `cases_reference_samples` and ask if the average score is above a threshold (default `0.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ebd46b-772a-4b71-af76-a5f2ab504c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Predicted Class: cases (0.5557858943939209)\n"
     ]
    }
   ],
   "source": [
    "# Select representative reference samples from each class\n",
    "num_reference_samples = 2  # CHANGE ME: Adjust the number of reference samples as needed\n",
    "\n",
    "# Randomly select reference samples from the \"cases\" class\n",
    "X_cases = features[labels==1]\n",
    "cases_reference_samples = X_cases[np.random.choice(len(X_cases), num_reference_samples, replace=False)].astype(float)\n",
    "\n",
    "# Grab a random sample to serve as \"new data\"\n",
    "new_data_sample = features[np.random.choice(len(features),  1)].astype(float).reshape(input_shape)\n",
    "\n",
    "# Define a function to compute the similarity score between a new data sample and reference samples\n",
    "def compute_similarity_scores(new_data, reference_samples):\n",
    "    # Initialize an array to store similarity scores\n",
    "    similarity_scores = []\n",
    "\n",
    "    for reference_sample in reference_samples:\n",
    "        # Ensure the data has the shape (1, input_dim) for prediction\n",
    "        new_data = np.reshape(new_data, (1, input_shape[0]))\n",
    "        reference_sample = np.reshape(reference_sample, (1, input_shape[0]))\n",
    "        \n",
    "        # Compute the similarity or dissimilarity score with the Siamese model\n",
    "        similarity_score = model.predict([new_data, reference_sample])\n",
    "\n",
    "        # Append the score to the list\n",
    "        similarity_scores.append(similarity_score)\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "# Compute similarity scores with reference samples for both classes\n",
    "cases_similarity_scores = compute_similarity_scores(new_data_sample, cases_reference_samples)\n",
    "\n",
    "# Calculate average similarity scores for each class\n",
    "average_cases_similarity = np.mean(cases_similarity_scores)\n",
    "\n",
    "# Set a threshold (adjust as needed)\n",
    "threshold = 0.5  # CHANGE ME\n",
    "\n",
    "# Make a binary classification decision based on the threshold\n",
    "if average_cases_similarity > threshold:\n",
    "    predicted_class = \"cases\"\n",
    "else:\n",
    "    predicted_class = \"controls\"\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class} ({average_cases_similarity})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c258b27-3555-4f2d-8f53-72469668a4c4",
   "metadata": {},
   "source": [
    "## Suggested Improvements\n",
    "\n",
    "- Use a proper data loader utility to dynamically pull/transform your pair wise data. Every possible combination of 2 in your pool of 400,000 is a VERY big number.\n",
    "    - You also don't have to use EVERY possible combination, you can generate random pairs if the data is too big\n",
    "- Extend data beyond binary classification\n",
    "    - Better to train a single big model properly, then a bunch of improperly/not tuned small models.\n",
    "- Pick an appropriate numerical encoding for your data to differentiate between `None` and `0`.\n",
    "- Tune neural network architecture\n",
    "    - Look at literature and borrow other architectures.\n",
    "    - Some, like ResNet, are automatically included with Tensorflow Hub and can be pulled with/without pre-trained weights\n",
    "- Tune hyperparameters\n",
    "- Chose more than 2 reference samples\n",
    "    - Ideally you pick a set of \"gold standard\" references\n",
    "- Tune inference threshold by inspecting distributions of scores\n",
    "- Refactor training using Ray for distributed computing\n",
    "    - Execute on cloud infrastructure w/ Ray Cluster: https://docs.ray.io/en/latest/cluster/getting-started.html\n",
    "    - Ray Train integration with Tensorflow: https://docs.ray.io/en/latest/train/distributed-tensorflow-keras.html\n",
    "    - Ray Tune Hyperparameter tuning integration with Optuna: https://docs.ray.io/en/latest/tune/index.html\n",
    "    - Ray Data for scalable data processing: https://docs.ray.io/en/latest/data/data.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
